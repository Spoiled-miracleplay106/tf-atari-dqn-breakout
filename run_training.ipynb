{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a DQN Agent for Atari Breakout\n",
    "\n",
    "This notebook provides a guided walkthrough for running the training script for our Deep Q-Learning (DQN) agent. \n",
    "\n",
    "All the core logic (the DQN model, replay buffer, and training loop) is neatly organized in the `src/` directory. The `train.py` script is the main entry point that brings all these modules together.\n",
    "\n",
    "For a complete theoretical breakdown of *how* this works (Q-Learning, Bellman Equation, Experience Replay, etc.), please see the `README.md` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies\n",
    "\n",
    "First, we need to install the necessary libraries. The `requirements.txt` file handles most of them, but the `gymnasium` Atari package requires a special installation command to accept the game ROM licenses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies from requirements.txt\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# Install gymnasium with Atari support AND accept the ROM license\n",
    "!pip install -q \"gymnasium[atari,accept-rom-license]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Running the Training Script\n",
    "\n",
    "We can now execute the `train.py` script directly from this notebook. \n",
    "\n",
    "We will run a **shorter training session** than the default to demonstrate the process. A full run to achieve high performance can take millions of steps (days of training). \n",
    "\n",
    "Here's a breakdown of the custom arguments we'll use:\n",
    "* `--num_episodes 1500`: Run for 1,500 episodes (a short demo).\n",
    "* `--replay_memory_init_size 10000`: Populate the buffer with 10k random steps before training (reduced from 50k for a quick start).\n",
    "* `--epsilon_decay_steps 100000`: Decay epsilon from 1.0 to 0.1 over 100k steps (reduced from 500k).\n",
    "* `--max_steps_per_episode 5000`: **Note:** The original notebook had this at 1000, which was too low and likely caused the agent to stop prematurely. We use a more reasonable 5,000 steps.\n",
    "* `--experiment_dir ./experiments/notebook_run`: Save logs and videos to a specific directory.\n",
    "\n",
    "Training will log to the console and to `logs/training.log`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py \\\n",
    "    --num_episodes 1500 \\\n",
    "    --replay_memory_init_size 10000 \\\n",
    "    --epsilon_decay_steps 100000 \\\n",
    "    --update_target_estimator_every 5000 \\\n",
    "    --max_steps_per_episode 5000 \\\n",
    "    --experiment_dir ./experiments/notebook_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Monitoring with TensorBoard\n",
    "\n",
    "The script saves all training metrics (reward, loss, episode length, etc.) to the `experiments/` directory. You can use TensorBoard to visualize this data in real-time.\n",
    "\n",
    "You can run the following command in a **separate terminal** from the `tf-atari-dqn-breakout` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You must run this from a separate terminal in the project's root directory\n",
    "# tensorboard --logdir=./experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the command, open your browser to `http://localhost:6006` to see the training graphs.\n",
    "\n",
    "You will see the metrics from our `notebook_run` and any other runs you've started."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
